{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for [MiniBatchSparsePCA on Text Data](https://stackoverflow.com/questions/48034724/minibatchsparsepca-on-text-data#comment83167351_48034724). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train = fetch_20newsgroups(subset='train')\n",
    "dataset_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = dataset_train['target']\n",
    "y_test = dataset_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary has  22955 entries\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5, token_pattern='[a-zA-Z]+', stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(dataset_train['data'])\n",
    "X_test = vectorizer.transform(dataset_test['data'])\n",
    "\n",
    "print('dictionary has ', len(vectorizer.vocabulary_), 'entries')\n",
    "\n",
    "X_train_dense = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 7.58 s, total: 35.3 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "\n",
    "spca = MiniBatchSparsePCA(n_components=n_components, alpha=0.04,\n",
    "                          batch_size=3, n_iter=100, random_state=0)\n",
    "\n",
    "%time X_train_reduced = spca.fit_transform(X_train_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component 0 has 31 nonzero entries\n",
      "component 1 has 20 nonzero entries\n",
      "component 2 has 28 nonzero entries\n",
      "component 3 has 33 nonzero entries\n",
      "component 4 has 34 nonzero entries\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_components):\n",
    "  print('component', i, 'has', sum(spca.components_[i, :] != 0), 'nonzero entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving indices of nonzero entries in components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "component_idxs = [np.where(spca.components_[i, :])[0] for i in range(n_components)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverting vocabulary (token -> int mapping) to int -> token mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_to_words = dict((n, word) for (word, n) in vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for 0 component:\n",
      "['address', 'ai', 'andrew', 'berkeley', 'ca', 'cable', 'card', 'chip', 'columbia', 'cubs', 'digex', 'dma', 'dos', 'doug', 'edu', 'ftp', 'g', 'game', 'james', 'marc', 'motorola', 'o', 'object', 'rose', 's', 'sandvik', 'space', 'state', 'su', 'uga', 'x']\n",
      "\n",
      "Tokens for 1 component:\n",
      "['acsu', 'behanna', 'buffalo', 'ca', 'cs', 'csc', 'data', 'ericsson', 'freenet', 'government', 'l', 'nasa', 'p', 'problem', 'thanks', 'usc', 'utexas', 'uucp', 'v', 'video']\n",
      "\n",
      "Tokens for 2 component:\n",
      "['andrew', 'au', 'berkeley', 'bus', 'c', 'cmu', 'color', 'cs', 'data', 'david', 'digital', 'dog', 'don', 'fpu', 'gun', 'll', 'lost', 'nasa', 'netcom', 'problem', 's', 'scsi', 'se', 'small', 'unit', 've', 'version', 'vram']\n",
      "\n",
      "Tokens for 3 component:\n",
      "['att', 'chip', 'christ', 'computer', 'cs', 'data', 'drive', 'fbi', 'file', 'files', 'game', 'god', 'hp', 'ibm', 'insurance', 'k', 'legal', 'lehigh', 'm', 'mouse', 'polygon', 'price', 'running', 's', 'season', 'simms', 'temple', 've', 'w', 'way', 'win', 'windows', 'x']\n",
      "\n",
      "Tokens for 4 component:\n",
      "['access', 'believe', 'blood', 'bmw', 'card', 'cd', 'center', 'cleveland', 'com', 'cwru', 'dos', 'drive', 'dyer', 'encryption', 'engr', 'file', 'god', 'hell', 'henry', 'information', 'israel', 'just', 'key', 'keys', 'king', 'mark', 'men', 'oracle', 'org', 'software', 'space', 'steve', 'upenn', 'writes']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print('Tokens for', i, 'component:')\n",
    "  print([idx_to_words[i] for i in component_idxs[i]])\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
